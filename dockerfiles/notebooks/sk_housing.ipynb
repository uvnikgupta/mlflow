{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sys import version_info\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "import dvc.api\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "backend_uri = os.environ['MLFLOW_TRACKING_URI']\n",
    "artifact_uri = os.environ['MLFLOW_ARTIFACT_STORE']\n",
    "mlflow.set_tracking_uri(backend_uri)\n",
    "\n",
    "PYTHON_VERSION = \"{major}.{minor}.{micro}\".format(major=version_info.major,\n",
    "                                                  minor=version_info.minor,\n",
    "                                                  micro=version_info.micro)\n",
    "pipeline_path = \"pipe.pkl\"\n",
    "model_path = \"model.pkl\"\n",
    "\n",
    "path = 'dataset/housing.csv'\n",
    "repo = '../.git'\n",
    "version = 'v1' #Git tag\n",
    "\n",
    "data_url = dvc.api.get_url(\n",
    "    path = path,\n",
    "    repo = repo,\n",
    "    rev = version\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def fetch_data_from_s3(path, repo, version):\n",
    "    data = dvc.api.read(\n",
    "            path = path,\n",
    "            repo = repo,\n",
    "            rev = version\n",
    "        )\n",
    "    return pd.read_csv(io.StringIO(data), sep=',')\n",
    "\n",
    "\n",
    "def fetch_data_from_fs(url):\n",
    "    return pd.read_csv(url, sep=',')\n",
    "\n",
    "\n",
    "def fetch_data(url):\n",
    "    storage_type = url.split(\":\")[0]\n",
    "    if storage_type.upper() == \"S3\":\n",
    "        data = fetch_data_from_s3(path, repo, version)\n",
    "    else:\n",
    "        data = fetch_data_from_fs(url)\n",
    "        \n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing = fetch_data(data_url)\n",
    "housing.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "targetCol = \"median_house_value\"\n",
    "catCols = [\"ocean_proximity\"]\n",
    "numCols = [\"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "                    (\"Imputer\", SimpleImputer()),\n",
    "                    (\"Scaler\", StandardScaler())\n",
    "                ])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "                    (\"Numerical_Pipeline\", num_pipeline, numCols),\n",
    "                    (\"OneHot\", OneHotEncoder(), catCols)\n",
    "                ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = housing[numCols + catCols]\n",
    "y = housing[targetCol]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "x_train = full_pipeline.fit_transform(x_train)\n",
    "x_test = full_pipeline.transform(x_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "categories = list(full_pipeline.named_transformers_['OneHot'].categories_[0])\n",
    "feature_names = numCols + categories"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "rmse"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the pipeline and the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "joblib.dump(model, model_path)\n",
    "joblib.dump(full_pipeline, pipeline_path);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create an `artifacts` dictionary that assigns a unique name to the saved pipeline and the model\n",
    "# This dictionary will be passed to `mlflow.pyfunc.save_model`, which will copy the model file\n",
    "# into the new MLflow Model's directory.\n",
    "artifacts = {\n",
    "    \"pipeline\": pipeline_path,\n",
    "    \"model\": model_path\n",
    "}\n",
    "\n",
    "# Define the model class\n",
    "class ModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        self.pipeline = joblib.load(context.artifacts[\"pipeline\"])\n",
    "        self.model = joblib.load(context.artifacts[\"model\"])\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        model_input.columns = [\"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \n",
    "                   \"population\", \"households\", \"median_income\", \"ocean_proximity\"]\n",
    "        input_matrix = self.pipeline.transform(model_input)\n",
    "        return self.model.predict(input_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a Conda environment for the new MLflow Model that contains all necessary dependencies.\n",
    "import cloudpickle\n",
    "conda_env = {\n",
    "    'channels': ['defaults'],\n",
    "    'dependencies': [\n",
    "      'python={}'.format(PYTHON_VERSION),\n",
    "      'pip',\n",
    "      {\n",
    "        'pip': [\n",
    "          'mlflow',\n",
    "          'scikit-learn=={}'.format(sklearn.__version__),\n",
    "          'joblib=={}'.format(joblib.__version__),\n",
    "          'cloudpickle=={}'.format(cloudpickle.__version__),\n",
    "        ],\n",
    "      },\n",
    "    ],\n",
    "    'name': 'model_env'\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_schema = []\r\n",
    "for col in numCols:\r\n",
    "    input_schema.append(ColSpec(\"double\", col))\r\n",
    "    \r\n",
    "for col in catCols:\r\n",
    "    input_schema.append(ColSpec(\"string\", col))\r\n",
    "    \r\n",
    "input_schema = Schema(input_schema)\r\n",
    "output_schema = Schema([ColSpec('double')])\r\n",
    "sign = ModelSignature(inputs=input_schema, outputs=output_schema)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "experiment_name = \"regres_sk_housing\"\r\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\r\n",
    "if experiment is None:\r\n",
    "    experiment_id = mlflow.create_experiment(experiment_name, artifact_location=artifact_uri)\r\n",
    "    experiment = mlflow.get_experiment(experiment_id)\r\n",
    "\r\n",
    "mlflow.set_experiment(experiment.name)\r\n",
    "\r\n",
    "mlflow_pyfunc_model_path = experiment_name\r\n",
    "with mlflow.start_run():\r\n",
    "    mlflow.log_param('data_url', data_url)\r\n",
    "    mlflow.log_param('data_version', version)\r\n",
    "    mlflow.log_param('input_rows', x_train.shape[0])\r\n",
    "    mlflow.log_param('input_columns', x_train.shape[1])\r\n",
    "    \r\n",
    "    mlflow.log_metric(\"rmse\", rmse)\r\n",
    "    mlflow.pyfunc.log_model(mlflow_pyfunc_model_path, \r\n",
    "                            python_model=ModelWrapper(), \r\n",
    "                            artifacts=artifacts,\r\n",
    "                            signature=sign,\r\n",
    "                            conda_env=conda_env)\r\n",
    "    mlflow.shap.log_explanation(model.predict, \r\n",
    "                                pd.DataFrame(data = x_test[:100], columns = feature_names))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the MLflow Model\r\n",
    "# mlflow.pyfunc.save_model(\r\n",
    "#         path=mlflow_pyfunc_model_path, python_model=ModelWrapper(), artifacts=artifacts,\r\n",
    "#         conda_env=conda_env)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}